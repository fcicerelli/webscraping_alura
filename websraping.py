# -*- coding: utf-8 -*-
"""websraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XFPJ8RBzWexTtXUjvBKmv-VLycgo19La

# <font color=green>1. MI PRIMER SCRAPING

# 1.1. Introducción

## *Web Scraping* es el término utilizado para definir la práctica de recolección automática de información en Internet. Esto se hace, generalmente, por medio de programas que simulan la navegación humana en la Web.

# 1.2. Ambiente y bibliotecas
### Utilizaremos en nuestro entrenamiento el navegador Google Chrome
"""

import bs4
import urllib.request as urllib_request
import pandas

print("BeautifulSoup ->", bs4.__version__)
print("urllib ->", urllib_request.__version__)
print("pandas ->", pandas.__version__)

"""# 1.3. Mi primer scraping"""

from bs4 import BeautifulSoup
from urllib.request import urlopen

url = "http://alura-latam-webscraping.website3.me/"

response = urlopen(url)
html = response.read()
soup = BeautifulSoup(html, "html.parser")
# También sirve soup.find("h2", {"class":"bb-section-title"})
# soup.find("p").get_text()

print(soup.find("h2", class_="bb-section-title").get_text())
print(soup.find_all("p")[1].get_text())

"""---
# <font color=green>2. OBTENIENDO Y TRATANDO EL CONTENIDO DE UN HTML

# 2.1. Entendiendo la web

<img src="https://github.com/ahcamachod/alura/blob/main/web.PNG?raw=1" width="700">

# 2.2. Obteniendo el contenido HTML de un website

# urllib.request
## https://docs.python.org/3/library/urllib.html
"""

from urllib.request import urlopen

url = 'https://carros.tucarro.com.co/atlantico/_all*payment*methods*discount_cash*discount'

response = urlopen(url)
html = response.read()
html

"""## https://docs.python.org/3/library/urllib.request.html#urllib.request.Request"""

from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError

url = 'https://www.alura.com.br'

headers = {'User-Agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Mobile Safari/537.36'}

try:
  req = Request(url, headers = headers)
  response = urlopen(req)
  print(response.read())

except HTTPError as e:
  print(e.status, e.reason)

except URLError as e:
  print(e.reason)

"""# 2.3. Tratamiento de string"""

from urllib.request import urlopen

url = "https://carros.tucarro.com.co/atlantico/_all*payment*methods*discount_cash*discount"

response = urlopen(url)
html = response.read()
html

"""### Convirtiendo el tipo bytes a string"""

type(html)

html = html.decode('utf-8')
type(html)

"""### Eliminando los caracteres de tabulación, salto de línea etc."""

html = ' '.join(html.split())
html

"""### Eliminando los espacios en blanco entre las TAGS"""

html.replace('> <', '><')

"""### Función de tratamiento de strings"""

def trata_html(input):
  return ' '.join(html.split()).replace('> <', '><')

html = trata_html(html)
html

"""# <font color=green>3. INTRODUCCIÓN AL BEAUTIFULSOUP

# 3.1. HTML de nuestra página

**HTML** (*HyperText Markup Language*) es un lenguaje de marcación compuesto por **tags** que determinan el papel que cada parte del documento va a asumir. Las **tags** son formadas por su nombre y atributos. Los atributos sirven para configurar y también modificar las características estándar de una **tag**.

## Estructura básica

```html
<html>
    <head>
        <meta charset="utf-8" />
        <title>Web Scraping con Alura</title>
    </head>
    <body>
        <div id="container">
            <h1>Alura</h1>
            <h2 class="formato">Cursos de Tecnología</h2>
            <p>Usted va a estudiar, practicar, discutir y aprender.</p>
            <a href="https://www.alura.com.br/">Haga click aquí</a>
        </div>
    </body>
</html>
```

```<html>``` - determina el inicio del documento.

```<head>``` - encabezado. Contiene información y configuración del documento.

```<body>``` - es el cuerpo del documento, donde todo el contenido es colocado. Esta es la parte visible en un navegador.

## Tags más comunes

```<div>``` - define una división de la página. Puede ser formateada de diversas maneras.

```<h1>, <h2>, <h3>, <h4>, <h5>, <h6>``` - marcadores de títulos.

```<p>``` - marcador de párrafo.

```<a>``` - hiperlink.

```<img>``` - exibición de imágenes.

```<table>``` - definición de tablas.

```<ul>, <li>``` - definición de listas.

# 3.2. Creando un objeto BeautifulSoup

## https://www.crummy.com/software/BeautifulSoup/

### Sobre parser ver: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parser-installation
"""

html

from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser')
type(soup)
print(soup.prettify())

"""# 3.3. Accesando tags"""

soup.html.head.title

soup.title

soup.html.body.header.div.a

soup.a

"""# 3.4. Accesando el contenido de las tags"""

soup.html.head.title

soup.html.body.header.div.a

soup.title.get_text()

soup.a.get_text()

soup.a.getText()

# soup = BeautifulSoup('<h1>Título 1</h1><h2>Título 1.1</h2><p>Contenido</p>')
# soup.get_text(' # ')

'''
from bs4 import BeautifulSoup

html = """
    <html>
        <body>
            <div id="container-a">
                <h1>Título A</h1>
                <h2 class="ref-a">Subtítulo A</h2>
                <p>Texto de contenido A</p>
                <div id="container-a-1">
                    <h1>Título A.1</h1>
                    <h2 class="ref-a">Subtítulo A.1</h2>
                    <p>Texto de contenido A.1</p>
                </div>
            </div>
        </body>
    </html>
"""
soup = BeautifulSoup(html, 'html.parser')
soup
'''

# soup.get_text(separator=' || ', strip=True)

"""# 3.5. Accesando los atributos de una tag"""

soup.img

soup.img.attrs

print(soup.find_all("img")[4])

soup.find_all("img")[4].attrs

soup.find_all("img")[4]['src']

soup.find_all("img")[4].get('class')[2]

soup.find_all("img")[4].attrs.keys()

soup.find_all("img")[4].attrs.values()

# 3 formas de acceder al atributo 'src'
sopa = BeautifulSoup('<img src="scraping.jpg"/>')

#sopa.img.get('src')
#sopa.img.attrs['src']
sopa.img['src']

"""---
# <font color=green>4. BUSCANDO CON EL BEAUTIFULSOUP

# 4.1. Los métodos *find()* y *findAll()*

- ### *find(tag, attributes, recursive, text, **kwargs)*

- ### *findAll(tag, attributes, recursive, text, limit, **kwargs)*

#### https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find
#### https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all

> **Observación:**
> - *findAll()* también puede ser utilizado como *find_all()*

### Método *find()*
"""

soup.find('img')

"""### Método *findAll()*"""

soup.find_all('img')

soup.find_all('img')[2]

"""### Comando equivalente al método *find()*"""

soup.findAll('img', limit=1)

"""### Atajo para el método *findAll()*"""

soup('img')

"""### Pasando listas de TAGs"""

soup.findAll(['h1','h2','h3','h4','h5','h6'])

"""### Utilizando el argumento *attributes*"""

soup.findAll('img',{'class':"lazy-loadable"})
# soup.find_all('img', class_='lazy-loadable')

soup.findAll('img', {'class':'ui-search-result-image__element shops__image-element lazy-loadable'})

for item in soup.findAll('img'):
  print(item.get('data-src'))

[item.get('data-src') for item in soup.findAll('img')]

"""### Buscando el contenido de una TAG"""

[item.get('src') for item in soup.findAll('img',{'class':"lazy-loadable"})]

[(item.get('src') or item.get('data-src')) for item in soup.findAll('img',{'class':"lazy-loadable"})]

"""### Utilizando directamente los atributos"""

soup.findAll('img', width="284")

"""### Cuidado con el atributo "class"
"""

# soup.findAll('img', class_="loading")

soup.findAll('img', class_="lazy-loadable")

imgs = [item.get('data-src') for item in soup.find_all('img', class_='lazy-loadable')]
imgs

"""### Obteniendo todo el contenido de texto de una página"""

soup.findAll(text=True)

# from IPython.core.display import display, HTML

# for img in imgs:
  # display(HTML("<img src=" + img + ">"))

"""# 4.2. Otros métodos de búsqueda

- ### *findParent(tag, attributes, text, **kwargs)*

- ### *findParents(tag, attributes, text, limit, **kwargs)*

#### https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-parents-and-find-parent

> **Observación:**
> - *findParent()* y *findParents()* también pueden ser utilizados como *find_parent()* y *find_parents()*, respectivamente.
---
- ### *findNextSibling(tag, attributes, text, **kwargs)*

- ### *findNextSiblings(tag, attributes, text, limit, **kwargs)*

- ### *findPreviousSibling(tag, attributes, text, **kwargs)*

- ### *findPreviousSiblings(tag, attributes, text, limit, **kwargs)*

#### https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-next-siblings-and-find-next-sibling
#### https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-previous-siblings-and-find-previous-sibling

> **Observación:**
> - *findNextSibling()*, *findNextSiblings()*, *findPreviousSibling()* y *findPreviousSiblings()* también pueden ser utilizados como *find_next_sibling()*, *find_next_siblings()*, *find_previous_sibling()* y *find_previous_siblings()*, respectivamente.
---
- ### *findNext(tag, attributes, text, **kwargs)*

- ### *findAllNext(tag, attributes, text, limit, **kwargs)*

- ### *findPrevious(tag, attributes, text, **kwargs)*

- ### *findAllPrevious(tag, attributes, text, limit, **kwargs)*

#### https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all-next-and-find-next
#### https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all-previous-and-find-previous

> **Observación:**
> - *findNext()*, *findAllNext()*, *findPrevious* y *findAllPrevious* también pueden ser utilizados como *find_next()*, *find_all_next()*, *find_previous()* y *find_all_previous()*, respectivamente.

## HTML de ejemplo para ilustrar la utilización de los métodos de búsqueda del BeautifulSoup

<img src="https://caelum-online-public.s3.amazonaws.com/1381-scraping/01/BeautifulSoup-method.png" width=80%>

---
## Resultado

<html>
    <body>
        <div id=“container-a”>
            <h1>Título A</h1>
            <h2 class="ref-a">Sub título A</h2>
            <p>Texto de contenido A</p>
        </div>
        <div id=“container-b”>
            <h1>Título B</h1>
            <h2 class="ref-b">Sub título B</h2>
            <p>Texto de contenido B</p>
        </div>
    </body>
</html>
"""

test_html = """ <html>
    <body>
        <div id=“container-a”>
            <h1>Título A</h1>
            <h2 class="ref-a">Sub título A</h2>
            <p>Texto de contenido A</p>
        </div>
        <div id=“container-b”>
            <h1>Título B</h1>
            <h2 class="ref-b">Sub título B</h2>
            <p>Texto de contenido B</p>
        </div>
    </body>
</html> """

"""### Tratamientos para la string HTML"""

def trata_html(input):
  return ' '.join(input.split()).replace('> <', '><')

html = trata_html(test_html)
html

"""### Creando el objeto BeautifulSoup"""

soup = BeautifulSoup(test_html, 'html.parser')
soup

"""## Parents"""

soup.find('h2')

soup.find('h2').find_parents()

# Da error de atributos
soup.findAll('h2').find_parent('div')
# Se debe especificar un elemento de la lista para aplicar el .find_parent()

soup.findAll('h2')[0].find_parent('div')

[item.find_parent('div') for item in soup.findAll('h2')]

for item in soup.findAll('h2'):
   print(item.find_parent('div'))

"""## Siblings"""

soup.find('h2').findNextSibling()

soup.find('h2').findPreviousSibling()

soup.find('p').findPreviousSiblings()

"""## Next y Previous"""

soup.find('h2').findNext()

soup.find('h2').findPrevious()

soup.find('h2').findAllNext()

# Ejercicio

from bs4 import BeautifulSoup

html_ejercicio = """
    <html>
        <body>
            <div id="container-a">
                <h1>Título A</h1>
                <h2 class="ref-a">Subtítulo A</h2>
                <p>Curso de</p>
                <p>Web</p>
                <p>Scraping</p>
                <p>con Python</p>
            </div>
        </body>
    </html>
"""
soup_ejercicio = BeautifulSoup(html_ejercicio, 'html.parser')
soup_ejercicio

"""# <font color=green>5. WEB SCRAPING DEL SITE TUCARRO.COM.CO - OBTENIENDO LOS DATOS DE UN ANUNCIO

# 5.1. Identificando y seleccionando los datos en el HTML

### Obteniendo el HTML y creando el objeto BeautifulSoup
"""

from urllib.request import urlopen
from bs4 import BeautifulSoup

url = "https://carros.tucarro.com.co/atlantico/_all*payment*methods*discount_cash*discount"
response = urlopen(url)
html = response.read()
soup = BeautifulSoup(html, 'html.parser')
soup

"""### Creando variables para almacenar la información"""

cards = []
card = {}

"""### Obteniendo los datos del primer CARD"""

# anuncio = soup.find('div', class_="andes-card andes-card--flat andes-card--default ui-search-result shops__cardStyles ui-search-result--mot andes-card--padding-default andes-card--animated")
anuncio = soup.find('div', class_="andes-card ui-search-result shops__cardStyles ui-search-result--mot andes-card--flat andes-card--padding-16 andes-card--animated")

anuncio

"""# 5.2. Obteniendo el MODELO del vehículo anunciado"""

anuncio.find('div', class_='ui-search-item__group ui-search-item__group--title shops__items-group')

anuncio.find('h2')

anuncio.find('h2').getText()

card['Modelo'] = anuncio.find('h2').getText()

card

"""### <font color=red>Resumen"""

# Modelo
card['Modelo'] = anuncio.find('h2').getText()

"""# 5.3. Obteniendo la INFORMACIÓN sobre el vehículo anunciado"""

infos = anuncio.find('div', class_="ui-search-result__content-wrapper shops__result-content-wrapper")
infos

# infos.find('span', class_="price-tag-fraction").get_text()
infos.find('span', class_="andes-money-amount__fraction").get_text()

infos.find_all('li', class_="ui-search-card-attributes__attribute")

infos.find_all('li', class_="ui-search-card-attributes__attribute")[0].get_text()

infos.find_all('li', class_="ui-search-card-attributes__attribute")[1].get_text()

# infos.find_all('span', class_="ui-search-item__group__element ui-search-item__location shops__items-group-details")[0].get_text()

infos.find('span', class_="ui-search-item__group__element ui-search-item__location shops__items-group-details").get_text()

"""### <font color=red>Resumen"""

# Información

# card['Valor'] = infos.find('span', class_="price-tag-fraction").get_text()
card['Valor'] = infos.find('span', class_="andes-money-amount__fraction").get_text()
card['Año'] = infos.find_all('li', class_="ui-search-card-attributes__attribute")[0].get_text()
card['Kilometraje'] = infos.find_all('li', class_="ui-search-card-attributes__attribute")[1].get_text()
card['Localización'] = infos.find('span', class_="ui-search-item__group__element ui-search-item__location shops__items-group-details").get_text()
card

"""### Ejercicio"""

from bs4 import BeautifulSoup

html_ejercicio_2 = """
    <html>
        <body>
            <div id="container-a">
                <h1>Curso de Web Scraping</h1>
                <h3>Entrenamiento Alura</h3>
                <p class="main descr">Curso para enseñar técnicas de recolección de datos en la web.</p>
            </div>
            <div id="tools">
                <p class="main lib-a">BeautifulSoup</p>
                <p class="libs">findAll()</p>
                <p class="libs">getText()</p>
                <p class="main lib-b">urllib.request</p>
                <p class="libs">urlopen()</p>
                <p class="main lib-c">pandas</p>
                <p class="libs">DataFrame()</p>
            </div>
        </body>
    </html>
"""
soup_ejercicio_2 = BeautifulSoup(html_ejercicio_2, 'html.parser')

# Obtener la siguiente salida:
  # BeautifulSoup
  # urllib.request
  # pandas

# 1
items_ejercio_2 = soup_ejercicio_2.find('div', {'id': 'tools'}).findAll('p', {'class': 'main'})
for item_ejercio_2 in items_ejercio_2:
    print(item_ejercio_2.getText())

# 2
items_ejercio_2 = soup_ejercicio_2.find('div', {'id': 'tools'}).findAll('p', {'class': ['lib-a', 'lib-b', 'lib-c']})
for item_ejercio_2 in items_ejercio_2:
    print(item_ejercio_2.getText())

# 3
items_ejercicio_2 = soup_ejercicio_2.find('div', {'id': 'tools'}).findAll('p', {'class': ['lib-a', 'lib-b', 'lib-c']})
for item_ejercicio_2 in items_ejercio_2:
    print(item_ejercicio_2.getText())

"""# 5.4 Creando un DataFrame con los datos recolectados de TUCARRO.COM.CO"""

card

import pandas as pd

dataset = pd.DataFrame(card.values(), card.keys())
dataset

dataset = pd.DataFrame(card.values(), card.keys()).T
dataset

dataset.to_csv('dataset.csv', sep=';', index=False, encoding='utf-8')

pd.read_csv('dataset.csv', sep=';')

"""### Ejercicio

"""

import pandas as pd

card_2 = {'value': 'R$ 1000',
  'name': 'Fusca',
  'motor': 'Motor 1.6',
  'description': 'Ano 1963 - 455.286 km',
  'location': 'Belo Horizonte - MG',
  'items': ['Rádio', 'Buzina']}

dataset_2 = pd.DataFrame.from_dict(card_2, orient='index').T
dataset_2

"""# 5.6. Obteniendo la FOTO del anuncio"""

image = anuncio.find('img')
image.get('data-src')

"""### Visualizando la FOTO en el notebook (extra)"""

from IPython.core.display import display, HTML

display(HTML("<img src=" + anuncio.find('img').get('data-src') + ">"))

"""### Rutina para accesar y guardar la FOTO del anuncio

## https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve
"""

from urllib.request import urlretrieve

image.get('data-src').split('/')[-1]

from os import mkdir
mkdir("/content/output")
mkdir("/content/output/img")
mkdir("/content/output/data")

urlretrieve(image.get('data-src'), '/content/output/img/' +image.get('data-src').split('/')[-1])

!pwd

"""### <font color=red>Resumen"""

image = anuncio.find('img')
image.get('data-src')

from urllib.request import urlretrieve

urlretrieve(image.get('data-src'), '/content/output/img/' +image.get('data-src').split('/')[-1])

"""# <font color=green>6. WEB SCRAPING DEL SITE TUCARRO.COM.CO - OBTENIENDO LOS DATOS DE TODOS LOS ANUNCIOS DE UNA PÁGINA

# 6.1. Identificando la información en el HTML
"""

anuncios = soup.find_all('div', class_="andes-card ui-search-result shops__cardStyles ui-search-result--mot andes-card--flat andes-card--padding-16 andes-card--animated")
anuncios

len(anuncios)

for anuncio in anuncios:
  print(str(anuncio) + "\n\n")

"""# 6.2. Creando una rutina de scraping"""

from urllib.request import urlopen, urlretrieve
from bs4 import BeautifulSoup
import pandas as pd

url = "https://carros.tucarro.com.co/atlantico/_all*payment*methods*discount_cash*discount"
response = urlopen(url)
htm = response.read()
soup = BeautifulSoup(html, 'html.parser')

anuncios = soup.find_all('div', class_="andes-card ui-search-result shops__cardStyles ui-search-result--mot andes-card--flat andes-card--padding-16 andes-card--animated")

cards = []

for anuncio in anuncios:
  card = {}
  card['Modelo'] = anuncio.find('h2').getText()
  card['Valor'] = infos.find('span', class_="andes-money-amount__fraction").get_text()
  card['Año'] = infos.find_all('li', class_="ui-search-card-attributes__attribute")[0].get_text()
  card['Kilometraje'] = infos.find_all('li', class_="ui-search-card-attributes__attribute")[1].get_text()
  card['Localización'] = infos.find('span', class_="ui-search-item__group__element ui-search-item__location shops__items-group-details").get_text()
  cards.append(card)
  # carga las imágenes en la carpeta /content/output/img/
  image = anuncio.find('img')
  image.get('data-src')
  urlretrieve(image.get('data-src'), '/content/output/img/' +image.get('data-src').split('/')[-1])

len(cards)

cards

dataset = pd.DataFrame(cards)
dataset

dataset.to_csv('/content/output/data/dataset.csv', sep=';', index=False, encoding='utf-8')

"""# <font color=green>7. WEB SCRAPING DEL SITE TUCARRO.COM.CO - OBTENIENDO LOS DATOS DE TODOS LOS ANUNCIOS DE UNA PÁGINA

# 7.1. Identificando la información en el HTML
"""

numeros = []

for n in range(1,22):
  numeros.append((48*n)+1)

numeros

paginas = []

for n in numeros:
  paginas.append(f'https://carros.tucarro.com.co/atlantico/_Desde_{n}_NoIndex_True_all*payment*methods*discount_cash*discount')

paginas

len(paginas)

"""# 7.2. Montando un drive para almacenar la información"""

from google.colab import drive
drive.mount('/content/drive')

"""# 7.3. Creando una rutina de scraping

##https://docs.python.org/3/library/time.html
"""

# Importar Bibliotecas

from urllib.request import urlopen, urlretrieve
from bs4 import BeautifulSoup
import pandas as pd
import time # Dar una pequeña pausa entre las interacciones por página

# Algoritmo para saber el número de página
numeros = []
for n in range(1,22):
  numeros.append((48*n)+1)

# Lista de URLs que estaremos raspando
paginas = []
paginas.append('https://carros.tucarro.com.co/atlantico/_all*payment*methods*discount_cash*discount')
for n in numeros:
  paginas.append(f'https://carros.tucarro.com.co/atlantico/_Desde_{n}_NoIndex_True_all*payment*methods*discount_cash*discount')

cards = []  # Lista vacía para almacenar cada diccionario con los datos de los vehículos

# Loop para crear el objeto BeautifulSoup en cada URL
for url in paginas:
  url = url
  response = urlopen(url)
  htlm = response.read()
  soup = BeautifulSoup(html, 'html.parser')
  anuncios = soup.find_all('div', class_="andes-card ui-search-result shops__cardStyles ui-search-result--mot andes-card--flat andes-card--padding-16 andes-card--animated")

  # loop para obtener los datos de cada card
  for anuncio in anuncios:
    card = {} # Siempre que se repite el Loop, el diccionario vuelve a quedar vacío
    card['Modelo'] = anuncio.find('h2').getText()
    card['Valor'] = infos.find('span', class_="andes-money-amount__fraction").get_text()
    card['Año'] = infos.find_all('li', class_="ui-search-card-attributes__attribute")[0].get_text()
    card['Kilometraje'] = infos.find_all('li', class_="ui-search-card-attributes__attribute")[1].get_text()
    card['Localización'] = infos.find('span', class_="ui-search-item__group__element ui-search-item__location shops__items-group-details").get_text()
    cards.append(card) # Almacenar cada diccionario en un sub-índice de la lista cards

    # Obtener la imagen y almacenarla en el Drive
    image = anuncio.find('img')
    image.get('data-src')
    urlretrieve(image.get('data-src'), '/content/drive/MyDrive/output/img/' +image.get('data-src').split('/')[-1])
    print(image.get('data-src'), '/content/drive/MyDrive/output/img/' +image.get('data-src').split('/')[-1])

  time.sleep(10) # 2 segundos de delay entre cada iteración

cards

len(cards)

paginas

dataset = pd.DataFrame(cards)
dataset

dataset.to_csv('/content/drive/MyDrive/output/data/dataset.csv', sep=';', index=False, encoding=('utf-8'))

import pandas as pd
base_de_datos = pd.read_csv('https://raw.githubusercontent.com/alura-cursos/formacao-data-science/master/movies.csv')
base_de_datos